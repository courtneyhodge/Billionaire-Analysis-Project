---
title: "Project EDA"
author: "Charlie Perez"
date: "2024-07-29"
output: html_document
---

```{r}
library(tidyverse)
billionaires <- read.csv("Billionaires Statistics Dataset.csv")
```
Don't really know how to do EDA / transformations in R tbh
```{r}
summary(billionaires)
```
Thoughts: drop country latitude/longitude, birthYear, birthMonth, birthDay, date, either industry or category (they are the same) and recode as a categorical instead of a variable, rank (rank is unhelpful and can be extrapolated from finalWorth), drop either source or organization.

Other things can be done later, but these all seem useless. If doing some work on countries, USA will probably skew it, so I'm not sure how useful country info will be in any case.

Thoughts: 

MLR: how to determine what country conditions are good for billionaires or not

Logistic: Could we predict whether the next billionaire would be self made?


# Data Cleaning and exploration

## dropping columns that are duplicates
```{r}
billionaires_clean <- billionaires %>% select(-c(latitude_country, longitude_country, birthDate, birthYear, rank, date, category))
```

questionable variables include source/organization (too spread out, no reason to use it in regression), residenceStateRegion (similar reasons), rank(can be extrapolated from finalWorth), title (hard to create categories, but possible)

## variable transformations!

industries to factor
```{r}
billionaires_clean %>% mutate(industries=as.factor(industries))
```
```{r}
billionaires_clean <- billionaires_clean %>% mutate(gdp_country=as.numeric(gsub("[$,]", "", gdp_country)))
```

finalWorth log transformation

```{r}
billionaires_clean <- mutate(billionaires_clean, log_worth=log(finalWorth/100))
```
```{r}
summary(billionaires_clean$log_worth)
```
```{r}
billionaires_clean <- mutate(billionaires_clean, log_country_gdp = log(gdp_country), log_country_pop = log(population_country))
```
```{r}
summary(billionaires_clean$log_country_gdp)
summary(billionaires_clean$log_country_pop)
```


## removing some nulls

```{r}
billionaires_no_nulls <- billionaires_clean %>% drop_na()
billionaires_no_nulls
```
Lose 243 rows here - as far as I can tell, these are mostly either missing birthdates or missing country statistics 

```{r}
billionaires_columns <- colnames(billionaires_clean)
billionaires_columns <- billionaires_columns[- c(18, 19)] #birthdate columns
billionaires_only_bday_nulls <- billionaires_clean %>% drop_na(billionaires_columns)
billionaires_only_bday_nulls
```
This only retains 10 additional rows, but still useful and can be cleaned more later

```{r}
colnames(billionaires_clean)
```
```{r}
table(billionaires_only_bday_nulls$selfMade)
```

# country stats dataframe + cleaning

```{r}
colnames(billionaires_clean)
```

```{r}
billionaires_by_country <- billionaires_clean %>%
  group_by(country) %>%
  summarize(
    count = n(),
    cpi_country = first(cpi_country),
    cpi_change_country = first(cpi_change_country),
    gdp_country = first(gdp_country),
    gross_tertiary_education_enrollment = first(gross_tertiary_education_enrollment),
    gross_primary_education_enrollment_country = first(gross_primary_education_enrollment_country),
    life_expectancy_country = first(life_expectancy_country),
    tax_revenue_country_country = first(tax_revenue_country_country),
    total_tax_rate_country = first(total_tax_rate_country),
    population_country = first(population_country),
    log_country_gdp = first(log_country_gdp),
    log_country_pop = first(log_country_pop)
  )
billionaires_by_country
```
```{r}
countries_no_nulls <- billionaires_by_country %>% drop_na()
countries_no_nulls
```
```{r}
countries_w_nulls <- anti_join(billionaires_by_country, countries_no_nulls, by = "country")
countries_w_nulls
```


So if we get rid of nulls we lose 15 countries - not ideal. I like the idea of replacing with the mean a little better, particularly important to handle Hong Kong / Taiwan and the 38 that have no country.

Ignore below this (also - it may not work uh oh)
```{r}
model_bils <- lm(count~.-country, data=countries_no_nulls)
summary(model_bils)
aic <- MASS::stepAIC(model_bils, direction='both', Trace=F)
summary(aic)
```
visualizations
```{r}
countries_no_nulls$gdp_per_capita = countries_no_nulls$gdp_country / countries_no_nulls$population_country
ggplot(countries_no_nulls, aes(x=count, y=gdp_country))+geom_point()+geom_smooth(method='lm')
```
```{r}
ggplot(countries_no_nulls, aes(x=count, y=gross_primary_education_enrollment_country))+geom_point()+geom_smooth(method='lm')
```
```{r}
ggplot(countries_no_nulls, aes(x=count, y=total_tax_rate_country))+geom_point()+geom_smooth(method='lm')
```


Use PCR for this
```{r}
library(pls)
pcareg<-pcr(count~.-country, data=countries_no_nulls, scale=T)
summary(pcareg)
```

```{r}
countries2 <- countries_no_nulls[,-1]
pca<-princomp(countries2, cor=T,fix_sign = T)
summary(pca)
pca_data<-data.frame(count=countries_no_nulls$count, pca$scores)
pcareg2<-lm(count~Comp.1+Comp.2+Comp.3+Comp.4+Comp.5+Comp.6+Comp.7+Comp.8+Comp.9,data = pca_data)
summary(pcareg2)
```

Quite honestly, this is pretty decent. Obviously the negative numbers are a bit of a problem, but the model seems to do quite well at predicting for countries with larger numbers of billionaires.

Need to do cross validation + residual analysis
Small enough dataset to do leave one out cross validation
```{r}
pcareg3<-pcr(count~.-country, data=countries_no_nulls,scale=TRUE)
summary(pcareg3)
cv<-pcr(count~.-country, data=countries_no_nulls,scale=TRUE,validation="LOO")
validationplot(cv,val.type = "RMSEP")
validationplot(cv,val.type = "MSEP")
```
Elbow point either at 3 6 or 10
```{r}
new_dat <- countries_no_nulls
new_dat
predictions3 = predict(pcareg3,new_dat, ncomp=3)
predictions6 = predict(pcareg3,new_dat, ncomp=6)
predictions10 = predict(pcareg3,new_dat, ncomp=10)
# predictions3 <- ifelse(predictions3 < 0, 0, predictions3)
# predictions7 <- ifelse(predictions7 < 0, 0, predictions7)
# predictions9 <- ifelse(predictions9 < 0, 0, predictions9)
library(Metrics)
mae3 = mae(countries_no_nulls$count, predictions3)
rmse3 = rmse(countries_no_nulls$count, predictions3)
mae6 = mae(countries_no_nulls$count, predictions6)
rmse6 = rmse(countries_no_nulls$count, predictions6)
mae10 = mae(countries_no_nulls$count, predictions10)
rmse10 = rmse(countries_no_nulls$count, predictions10)
mae3
rmse3
mae6
rmse6
mae10
rmse10
```
One more thought - how does model performance change if we eliminate certain outliers?

```{r}
# remove Argentina, Nigeria, Egypt b/c they have really high inflation and CPI
countries4 = countries_no_nulls[-c(2,17,39),]
countries4
```
```{r}
pcareg4<-pcr(count~.-country, data=countries4,scale=TRUE)
summary(pcareg4)
cv<-pcr(count~.-country, data=countries4,scale=TRUE,validation="LOO")
validationplot(cv,val.type = "RMSEP")
validationplot(cv,val.type = "MSEP")
```

```{r}
new_dat <- countries4
new_dat
predictions7 = predict(pcareg4,new_dat, ncomp=7)
predictions10 = predict(pcareg4,new_dat, ncomp=10)
mae7 = mae(countries4$count, predictions7)
rmse7 = rmse(countries4$count, predictions7)
mae10 = mae(countries4$count, predictions10)
rmse10 = rmse(countries4$count, predictions10)
mae7
rmse7
mae10
rmse10
```
```{r}
# table for visual presentation
residuals <- countries4$count - predictions9
residuals <- round(residuals)
predictions9 <- round(predictions9)
countries_df <- countries4[c(3, 5, 11, 12, 22, 26, 36, 43, 45, 46, 50, 51, 58, 59),c(1,2,3,5,10, 11)]
countries_df$prediction <- predictions9[c(3, 5, 11, 12, 22, 26, 36, 43, 45, 46, 50, 51, 58, 59)]
countries_df$residual <- residuals[c(3, 5, 11, 12, 22, 26, 36, 43, 45, 46, 50, 51, 58, 59)]
countries_df <- countries_df[,c(1,2,7,8,3,4,5,6)]
countries_df
view(countries_df)
```



